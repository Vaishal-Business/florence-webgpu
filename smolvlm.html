<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>SmolVLM WebGPU Full Demo</title>
<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
<style>
  body { font-family: Arial; padding: 20px; background: #111; color: #eee; }
  input, button { margin: 10px 0; }
  img { max-width: 300px; display: block; margin: 10px 0; }
  pre { background: #222; padding: 10px; white-space: pre-wrap; word-wrap: break-word; }
</style>
</head>
<body>

<h1>SmolVLM WebGPU Full Demo</h1>

<input type="file" id="imageUpload" accept="image/*"><br>
<input type="text" id="prompt" placeholder="Enter your prompt" value="extract the text from image accurately"><br>
<button id="runBtn">Run Model</button>

<img id="preview" src="" alt="Uploaded image preview">
<pre id="output">Loading models...</pre>

<script>
let visionSession = null;
let embedSession = null;
let decoderSession = null;

// ONNX URLs
const visionURL = 'https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct/resolve/main/onnx/vision_encoder_q4f16.onnx';
const embedURL = 'https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct/resolve/main/onnx/embed_tokens.onnx';
const decoderURL = 'https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct/resolve/main/onnx/decoder_model_merged_q4f16.onnx';

// Load all models
async function loadModels() {
    const outputEl = document.getElementById('output');
    outputEl.textContent = 'Loading vision encoder...';
    visionSession = await ort.InferenceSession.create(visionURL, {executionProviders: ['webgpu','wasm'], gpuOptions:{useBufferMapping:false}});
    outputEl.textContent = 'Loading text embedding model...';
    embedSession = await ort.InferenceSession.create(embedURL, {executionProviders: ['webgpu','wasm'], gpuOptions:{useBufferMapping:false}});
    outputEl.textContent = 'Loading decoder...';
    decoderSession = await ort.InferenceSession.create(decoderURL, {executionProviders: ['webgpu','wasm'], gpuOptions:{useBufferMapping:false}});
    outputEl.textContent = 'Models loaded! Upload an image and enter a prompt.';
}

loadModels();

// Image upload preview
document.getElementById('imageUpload').addEventListener('change', (evt) => {
    const file = evt.target.files[0];
    if (!file) return;
    const img = document.getElementById('preview');
    img.src = URL.createObjectURL(file);
});

// Resize image to 160x160
function resizeImage(img) {
    const canvas = document.createElement('canvas');
    const size = 160;
    canvas.width = size;
    canvas.height = size;
    const ctx = canvas.getContext('2d');
    ctx.drawImage(img, 0, 0, size, size);
    return ctx.getImageData(0, 0, size, size);
}

// Convert ImageData to Float32Array (C,H,W)
function imageDataToTensor(imageData) {
    const { data, width, height } = imageData;
    const floatData = new Float32Array(3 * width * height);
    for (let y=0; y<height; y++){
        for (let x=0; x<width; x++){
            const idx = (y*width + x)*4;
            const i = y*width + x;
            floatData[i] = data[idx]/255;                 // R
            floatData[width*height + i] = data[idx+1]/255; // G
            floatData[2*width*height + i] = data[idx+2]/255; // B
        }
    }
    return floatData;
}

// Simple tokenizer (dummy)
function simpleTokenizer(text, maxLen=8){
    const tokens = text.split(' ').slice(0,maxLen).map((_,i)=>BigInt(i+1));
    while(tokens.length<maxLen) tokens.push(0n);
    return new BigInt64Array(tokens);
}

// Run model
document.getElementById('runBtn').addEventListener('click', async () => {
    const outputEl = document.getElementById('output');

    if(!visionSession || !embedSession || !decoderSession){
        alert('Models not loaded yet!');
        return;
    }

    const imgEl = document.getElementById('preview');
    if(!imgEl.src){
        alert('Upload an image first!');
        return;
    }

    const prompt = document.getElementById('prompt').value || "extract the text from image accurately";

    // Wait image load
    const img = new Image();
    img.src = imgEl.src;
    await new Promise(r=>img.onload=r);

    // Prepare image
    const imageData = resizeImage(img);
    const pixelValues = imageDataToTensor(imageData);

    // Vision encoder
    const visionFeeds = { pixel_values: new ort.Tensor('float32', pixelValues, [1,3,160,160]) };
    const visionResult = await visionSession.run(visionFeeds);
    const visionKey = Object.keys(visionResult)[0];
    const visionEmbeds = visionResult[visionKey]; // ort.Tensor

    // Pixel attention mask (all ones)
    const numPatches = visionEmbeds.dims[1];
    const pixelAttentionMask = new BigInt64Array(numPatches);
    for(let i=0;i<numPatches;i++) pixelAttentionMask[i] = 1n;

    // Text embedding
    const maxLen = 8;
    const inputIds = simpleTokenizer(prompt, maxLen);
    const attentionMask = new BigInt64Array(maxLen);
    for(let i=0;i<maxLen;i++) attentionMask[i] = inputIds[i]===0n ? 0n : 1n;

    const embedFeeds = {
        input_ids: new ort.Tensor('int64', inputIds, [1,maxLen]),
        attention_mask: new ort.Tensor('int64', attentionMask, [1,maxLen])
    };
    const embedResult = await embedSession.run(embedFeeds);
    const embedKey = Object.keys(embedResult)[0];
    const inputsEmbeds = embedResult[embedKey];

    // Decoder
    const decoderFeeds = {
        pixel_values: visionEmbeds,
        inputs_embeds: inputsEmbeds,
        attention_mask: new ort.Tensor('int64', attentionMask, [1,maxLen])
    };

    const t0 = performance.now();
    const decoderResult = await decoderSession.run(decoderFeeds);
    const t1 = performance.now();

    outputEl.textContent =
        `Inference done in ${(t1-t0).toFixed(2)} ms\n`+
        `Decoder output keys: ${Object.keys(decoderResult).join(', ')}`;
});
</script>
</body>
</html>
