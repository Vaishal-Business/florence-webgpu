<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>SmolVLM WebGPU Demo</title>
<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
<style>
  body { font-family: Arial; padding: 20px; background: #111; color: #eee; }
  input, button { margin: 10px 0; }
  img { max-width: 300px; display: block; margin: 10px 0; }
  pre { background: #222; padding: 10px; }
</style>
</head>
<body>

<h1>SmolVLM WebGPU Demo</h1>

<input type="file" id="imageUpload" accept="image/*"><br>
<input type="text" id="prompt" placeholder="Enter your prompt" value="extract the text from image accurately"><br>
<button id="runBtn">Run Model</button>

<img id="preview" src="" alt="Uploaded image preview">
<pre id="output">Loading models...</pre>

<script>
let embedSession = null;
let decoderSession = null;

// URLs of ONNX files
const embedURL = 'https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct/resolve/main/onnx/embed_tokens.onnx';
const decoderURL = 'https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct/resolve/main/onnx/decoder_model_merged_q4f16.onnx';

async function loadModels() {
    document.getElementById('output').textContent = 'Loading embedding model...';
    embedSession = await ort.InferenceSession.create(embedURL, { executionProviders: ['webgpu'] });
    document.getElementById('output').textContent = 'Loading decoder model...';
    decoderSession = await ort.InferenceSession.create(decoderURL, { executionProviders: ['webgpu'] });
    document.getElementById('output').textContent = 'Models loaded! Upload an image and enter prompt.';
}

loadModels();

// Handle image upload preview
document.getElementById('imageUpload').addEventListener('change', (evt) => {
    const file = evt.target.files[0];
    if (!file) return;
    const img = document.getElementById('preview');
    img.src = URL.createObjectURL(file);
});

// Resize image to 160x160
function resizeImage(img) {
    const canvas = document.createElement('canvas');
    const size = 160;
    canvas.width = size;
    canvas.height = size;
    const ctx = canvas.getContext('2d');
    ctx.drawImage(img, 0, 0, size, size);
    return ctx.getImageData(0, 0, size, size);
}

// Convert ImageData to Float32Array (C,H,W)
function imageDataToTensor(imageData) {
    const { data, width, height } = imageData;
    const floatData = new Float32Array(3 * width * height);
    for (let y = 0; y < height; y++) {
        for (let x = 0; x < width; x++) {
            const idx = (y * width + x) * 4;
            const i = y * width + x;
            floatData[i] = data[idx] / 255;                 // R
            floatData[width*height + i] = data[idx+1] / 255; // G
            floatData[2*width*height + i] = data[idx+2] / 255; // B
        }
    }
    return floatData;
}

// Dummy tokenizer (split by space, map each word to an integer)
// Real SmolVLM tokenizer can be implemented or replaced with a small mapping
function simpleTokenizer(text, maxLen=16) {
    const tokens = text.split(' ').slice(0, maxLen).map((_,i) => BigInt(i+1));
    while (tokens.length < maxLen) tokens.push(0n); // pad
    return new BigInt64Array(tokens);
}

document.getElementById('runBtn').addEventListener('click', async () => {
    if (!embedSession || !decoderSession) {
        alert('Models not loaded yet!');
        return;
    }

    const imgEl = document.getElementById('preview');
    if (!imgEl.src) {
        alert('Upload an image first!');
        return;
    }

    const prompt = document.getElementById('prompt').value || "extract the text from image accurately";

    // Wait for image to load
    const img = new Image();
    img.src = imgEl.src;
    await new Promise(r => img.onload = r);

    // Image tensor
    const imageData = resizeImage(img);
    const pixelValues = imageDataToTensor(imageData);

    // Tokenize prompt
    const inputIds = simpleTokenizer(prompt, 16);

    // Get embeddings
    const embedFeeds = { input_ids: new ort.Tensor('int64', inputIds, [1,16]) };
    const embedResult = await embedSession.run(embedFeeds);
    const inputsEmbeds = embedResult.output; // 'output' key from embedding model

    // Feed decoder model
    const decoderFeeds = {
        pixel_values: new ort.Tensor('float32', pixelValues, [1,3,160,160]),
        inputs_embeds: inputsEmbeds
    };

    const t0 = performance.now();
    const decoderResult = await decoderSession.run(decoderFeeds);
    const t1 = performance.now();

    document.getElementById('output').textContent =
        `Inference done in ${(t1 - t0).toFixed(2)} ms\n` +
        `Decoder output keys: ${Object.keys(decoderResult).join(', ')}`;
});
</script>

</body>
</html>
